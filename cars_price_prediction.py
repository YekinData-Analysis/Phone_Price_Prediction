# -*- coding: utf-8 -*-
"""Cars Price Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o_WOMrBiRP9lhCv-xwnrXWELCLTbvgcR
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import files
uploaded=files.upload()

df=pd.read_csv('Car details v3.csv')

df.info()

df.head()

df=df.dropna()

df.head()

from sklearn.preprocessing import LabelEncoder

Code=LabelEncoder()

df['fuel']=Code.fit_transform(df['fuel'])
df['seller_type']=Code.fit_transform(df['seats'])
df['transmission']=Code.fit_transform(df['transmission'])
df['owner']=Code.fit_transform(df['owner'])
df['mileage']=Code.fit_transform(df['mileage'])
df['torque']=Code.fit_transform(df['torque'])

df.head()

df['engine']=df['engine'].str.split(' ',expand=True).drop(1,1)

df.head()

df['max_power']=df['max_power'].str.split(' ',expand=True).drop(1,1)

df.head()

df['name']=Code.fit_transform(df['name'])

df.head()

plt.figure(figsize=(15,10))
mask=np.triu(np.ones_like(df.corr(),dtype=bool))
sns.heatmap(df.corr(),annot=True,mask=mask,cmap='mako')

"""seats and Seller_type have high level of correlation. so we have to one of the columns."""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score,KFold
from sklearn.ensemble import BaggingRegressor,ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor

X=df.drop(['selling_price','seats'],axis=1)

y=df['selling_price']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)

X['engine']=X['engine'].astype(int)
X['max_power']=pd.to_numeric(X['max_power'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)

models=[]
models.append(('LR',LinearRegression()))
models.append(('BR',BaggingRegressor()))
models.append(('ER',ExtraTreesRegressor()))
models.append(('DTR',DecisionTreeRegressor()))

import sklearn

sklearn.metrics.get_scorer_names()

results=[]
names=[]
scoring='neg_mean_squared_error'

for name,model in models:
  cv_result=cross_val_score(model,X_train,y_train,cv=5,scoring=scoring)
  results.append(cv_result)
  names.append(name)
  pass_it="%s: %f (%f)"% (name,cv_result.mean(),cv_result.std())
  print(pass_it)

"""the Linear Regession lowest negative mean Squared error is performing poorly Here."""

#plotting out the models


fig=plt.figure()
fig.suptitle('Algorithms Comparison')
ax=fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)

"""Linear Regression Is Doing so poorly, while ExtraRegression,BaggingRegression and Decision Tress is doing so Good."""

#let Standardized it, if is going to increase the accuracy

from sklearn.preprocessing import StandardScaler

scale=StandardScaler()
scale.fit(X)
X=scale.transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)

model=ExtraTreesRegressor()
model.fit(X_train,y_train)
y_pred=model.predict(X_test)
print(model.score(X_train,y_train))
print(model.score(X_test,y_test))

#importing the metrics

from sklearn.metrics import r2_score

#model Accuracy

print(r2_score(y_pred,y_test))

#Error Rate


1-r2_score(y_pred,y_test)

data=pd.DataFrame({'Actual_value':y_test,'Predict_Values':y_pred})

data

from sklearn.metrics import mean_squared_error,mean_absolute_error

mae=mean_absolute_error(y_test,y_pred)
mae

"""We have a large error rate, this is as result extreme figures in Target Variable."""

#let plot it out

df['selling_price'].plot.box(vert=False)

"""Exactly, large number of larges values"""

#let Take out the outliers. by creating a varaible function


def outliers(data,columns):
  q1=df[columns].quantile(.25)
  q3=df[columns].quantile(.75)
  IQR=q3-q1
  lb=q1-(1.5*IQR)
  up=q3+(1.5*IQR)
  ls=df.index[(df[columns]<lb)|(df[columns]>up)]
  return ls
outt=['selling_price']
index_list=[]
for col in outt:
  index_list.extend(outliers(df,col))
index_list=sorted(set(index_list))
before_remove = df.shape

df =df.drop(index_list)
after_remove = df.shape

print(f'''Shape of data before removing outliers : {before_remove}
Shape of data after remove : {after_remove}''')

df['selling_price'].plot.box(vert=False)

"""Much better! let retain and retest"""

df.head()

#Let even see the prices differences with the max and the min

np.ptp(df['selling_price'])

"""The differences between the minimum target value and the maximum target values is much"""

X=df.drop(['selling_price','seats'],axis=1)

y=df['selling_price']

scale.fit(X)
X=scale.transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)

Ex_model=ExtraTreesRegressor()
Ex_model.fit(X_train,y_train)
y_prediction=Ex_model.predict(X_test)
print(f'Accuracy :',r2_score(y_test,y_prediction))

"""Waoh, even After we retraning it, our accuracy is Low compare to what we have earlier,

So Let Check for the Mean_absolute_error
"""

mae1=mean_absolute_error(y_test,y_prediction)
mae1

"""Waoh, what happen ?

when the Model have better Accuarcy scores and the root square scores is Great and solid the mean_absulte error seems to be high that is the prediction value is actually far from prediction. so we have to control the erroe rate

let plot the residual
"""

residual=y_test-y_prediction

residual

#LET RETRAIN, this time around we using another approach


X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.3, random_state=1)
X_other,X_test,y_others,y_test=train_test_split(X,y,test_size=0.2, random_state=1)

ex_model=ExtraTreesRegressor()
ex_model.fit(X_train,y_train)
testing=ex_model.predict(X_eval)
mse_2=mean_absolute_error(y_eval,testing)
mse_2

"""The mean_squared_error is dropping Little by Little.

now we gat 53974
"""

dframe=pd.DataFrame({'Actual_value':y_eval,'Prediction_Value':testing})

resi_test=(y_eval-testing)
resi_test

plt.figure(figsize=(12,6))
mask=np.triu(np.ones_like(df.corr(),dtype=bool))
sns.heatmap(df.corr(),mask=mask,annot=True)

df.head()

dframe

df.columns

X=df.drop(['selling_price','seats'],axis=1)

X.columns

#we have to reduce the error rate So well,

pd.DataFrame(data=ex_model.feature_importances_,columns=['Feature_Important'],index=X.columns).sort_values(by=['Feature_Important'],ascending=False)

"""noise is too Unnecessary variable is much on this dataset that i felt they are contributing the model development not not many

so if the columns important is < 0.05 am dropping such columns and re run the Model.if the will also help in the controlling The error rate.
"""

#right naw we selecting
dframe=df.copy()

dframe.head()

new_df=dframe.loc[:,['name','year','selling_price','max_power','torque','engine']]

new_df

X=new_df.drop('selling_price',axis=1)

y=new_df['selling_price']

scale=StandardScaler()
scale.fit(X)
X=scale.transform(X)

#let retrain

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=72)
X_other,X_test,y_others,y_test=train_test_split(X,y,test_size=0.2, random_state=72)

ex_model=ExtraTreesRegressor()
ex_model.fit(X_train,y_train)
testing=ex_model.predict(X_eval)
mse_2=mean_absolute_error(y_eval,testing)
mse_2

"""WE carry Out so many activvity trying to control The error rate.

but with the look of thing i was expecting with the proper selection of our columns according to the features important, we should be to attain lower error rate but rather we gat high errorate but slighly different from first one.
"""

#let try Another model

bg_model=BaggingRegressor()
bg_model.fit(X_train,y_train)
test_pred=bg_model.predict(X_eval)
mse3=mean_absolute_error(y_eval,test_pred)
mse3

"""Fuck! okay we go with ExtraRegressor. becus so far it the one That have gat the Minimum Errorate."""

def outliers(data,columns):
  q1=df[columns].quantile(.25)
  q3=df[columns].quantile(.75)
  IQR=q3-q1
  lb=q1-(1.5*IQR)
  up=q3+(1.5*IQR)
  ls=df.index[(df[columns]<lb)|(df[columns]>up)]
  return ls
outt=['selling_price']
index_list=[]
for col in outt:
  index_list.extend(outliers(df,col))
index_list=sorted(set(index_list))
before_remove = df.shape

df =df.drop(index_list)
after_remove = df.shape

print(f'''Shape of data before removing outliers : {before_remove}
Shape of data after remove : {after_remove}''')

df['selling_price'].plot.box(vert=False)

"""This Is Much better...

Let re retrain.
"""

df.head()

X=df.drop(['seats','selling_price'],axis=1)

y=df['selling_price']

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.3, random_state=1)
X_other,X_test,y_others,y_test=train_test_split(X,y,test_size=0.2, random_state=1)

lmodel=ExtraTreesRegressor()
lmodel.fit(X_train,y_train)
testing=lmodel.predict(X_eval)
mse_5=mean_absolute_error(y_eval,testing)
mse_5

"""lmodel is doing the lowest in town naw."""

#lett check for the accuracy

print(f'Model Training Scores is:',lmodel.score(X_train,y_train))
print(f'Model Testing Scores is :',lmodel.score(X_eval,y_eval))
print(f'Model Root squared is :',r2_score(y_eval,testing))

#let save this last model

import pickle as pk

with open('lmodel.pk','wb') as files:
  pk.dump(lmodel,files)

pip install comet_ml

#let register the model

import comet_ml
experiment = comet_ml.Experiment(
    api_key="dTnbSNs6NzPtUbKKERUdq6NvO",
    project_name="Cars Price Prediction.ipynb",workspace='yek50')

